#:schema https://github.com/liblaf/llm-cli/raw/refs/heads/main/docs/schema/config.json

# litellm.CompletionRequest
[completion]
model = "deepseek-chat"

# litellm.RouterConfig
[[router.model_list]]
model_name = "deepseek-chat"
litellm_params.api_key = '{{ (rbwFields "DeepSeek").api_key.value }}'
litellm_params.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
litellm_params.model = "deepseek/deepseek-chat"

[[router.model_list]]
model_name = "deepseek-v3"
litellm_params.api_key = '{{ (rbwFields "Aliyun").api_key.value }}'
litellm_params.model = "openai/deepseek-v3"

[[router.model_list]]
model_name = "qwen-max"
litellm_params.api_key = '{{ (rbwFields "Aliyun").api_key.value }}'
litellm_params.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
litellm_params.model = "openai/qwen-max"

[[router.model_list]]
model_name = "qwen-plus"
litellm_params.api_key = '{{ (rbwFields "Aliyun").api_key.value }}'
litellm_params.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
litellm_params.model = "openai/qwen-plus"

[[router.model_list]]
model_name = "qwen-turbo"
litellm_params.api_key = '{{ (rbwFields "Aliyun").api_key.value }}'
litellm_params.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
litellm_params.model = "openai/qwen-turbo"

[[router.model_list]]
model_name = "qwen-long"
litellm_params.api_key = '{{ (rbwFields "Aliyun").api_key.value }}'
litellm_params.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
litellm_params.model = "openai/qwen-long"

[[router.fallbacks]]
deepseek-chat = ["qwen-long", "qwen-max", "qwen-plus", "qwen-turbo"]

[[router.fallbacks]]
qwen-max = ["qwen-long", "qwen-plus", "qwen-turbo"]
